{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Q&A Systems - Evaluation\n",
    "\n",
    "This notebook compares the performance of the RAG and Fine-Tuned models for financial Q&A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules with detailed error handling\n",
    "print(\"üîÑ Loading evaluation modules...\")\n",
    "import_status = {\"rag\": False, \"fine_tuner\": False, \"evaluator\": False}\n",
    "\n",
    "try:\n",
    "    from src.rag_system.integrated_rag import IntegratedRAG as RAG\n",
    "    import_status[\"rag\"] = True\n",
    "    print(\"‚úÖ RAG system imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå RAG import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.fine_tuning.fine_tuner import FineTuner\n",
    "    import_status[\"fine_tuner\"] = True\n",
    "    print(\"‚úÖ FineTuner imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå FineTuner import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    from src.evaluation.evaluator import Evaluator\n",
    "    import_status[\"evaluator\"] = True\n",
    "    print(\"‚úÖ Evaluator imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Evaluator import failed: {e}\")\n",
    "\n",
    "# Summary\n",
    "successful_imports = sum(import_status.values())\n",
    "total_imports = len(import_status)\n",
    "\n",
    "if successful_imports == total_imports:\n",
    "    print(\"üéâ All evaluation imports successful!\")\n",
    "elif successful_imports > 0:\n",
    "    print(f\"‚ö†Ô∏è Partial success: {successful_imports}/{total_imports} imports successful\")\n",
    "    print(\"üìù Some functionality may be limited\")\n",
    "else:\n",
    "    print(\"‚ùå All imports failed. Please check your environment setup.\")\n",
    "\n",
    "print(f\"Import status: RAG={import_status['rag']}, FineTuner={import_status['fine_tuner']}, Evaluator={import_status['evaluator']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = project_root / \"data\"\n",
    "QA_PAIRS_DIR = DATA_DIR / \"qa_pairs\"\n",
    "RAG_MODEL_DIR = project_root / \"models\" / \"rag\"\n",
    "FT_MODEL_DIR = project_root / \"models\" / \"fine_tuned\"\n",
    "EVALUATION_DIR = project_root / \"evaluation_results\"\n",
    "\n",
    "# Create evaluation directory if it doesn't exist\n",
    "EVALUATION_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Models\n",
    "\n",
    "First, let's load both the RAG and Fine-Tuned models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RAG system (only if imported successfully)\n",
    "if import_status[\"rag\"]:\n",
    "    print(\"Loading RAG system...\")\n",
    "    try:\n",
    "        rag_system = RAG()\n",
    "        if RAG_MODEL_DIR.exists():\n",
    "            rag_system.load(RAG_MODEL_DIR)\n",
    "            print(\"‚úÖ RAG system loaded from saved model\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No saved RAG model found. Creating new instance...\")\n",
    "            # Initialize a new RAG system for evaluation\n",
    "            rag_system = RAG(\n",
    "                embedding_model=\"all-MiniLM-L6-v2\",\n",
    "                llm_model=\"distilgpt2\",\n",
    "                chunk_sizes=[100, 400],\n",
    "                chunk_overlap=50,\n",
    "                retrieval_method=\"hybrid\",\n",
    "                top_k=3\n",
    "            )\n",
    "            print(\"‚úÖ New RAG system created\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with RAG system: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        rag_system = None\n",
    "else:\n",
    "    print(\"‚ùå Skipping RAG system loading - import failed\")\n",
    "    rag_system = None\n",
    "\n",
    "# Load the Fine-Tuned model (only if imported successfully)\n",
    "if import_status[\"fine_tuner\"]:\n",
    "    print(\"\\nLoading Fine-Tuned model...\")\n",
    "    try:\n",
    "        if FT_MODEL_DIR.exists():\n",
    "            ft_model = FineTuner(output_dir=FT_MODEL_DIR)\n",
    "            print(\"‚úÖ Fine-Tuned model loaded\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No saved fine-tuned model found. Creating new instance...\")\n",
    "            ft_model = FineTuner(\n",
    "                model_name=\"distilgpt2\",\n",
    "                output_dir=FT_MODEL_DIR,\n",
    "                use_peft=True\n",
    "            )\n",
    "            print(\"‚úÖ New Fine-Tuner created\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading Fine-Tuned model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        ft_model = None\n",
    "else:\n",
    "    print(\"‚ùå Skipping Fine-Tuned model loading - import failed\")\n",
    "    ft_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Evaluator\n",
    "\n",
    "Now, let's initialize the evaluator with both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "print(\"Initializing evaluator...\")\n",
    "try:\n",
    "    evaluator = Evaluator(\n",
    "        rag_system=rag_system,\n",
    "        ft_model=ft_model,\n",
    "        output_dir=EVALUATION_DIR\n",
    "    )\n",
    "    print(\"‚úÖ Evaluator initialized\")\n",
    "    \n",
    "    # Check which models are available for evaluation\n",
    "    if rag_system:\n",
    "        print(\"‚úÖ RAG system available for evaluation\")\n",
    "    else:\n",
    "        print(\"‚ùå RAG system not available\")\n",
    "        \n",
    "    if ft_model:\n",
    "        print(\"‚úÖ Fine-tuned model available for evaluation\")\n",
    "    else:\n",
    "        print(\"‚ùå Fine-tuned model not available\")\n",
    "        \n",
    "    if not rag_system and not ft_model:\n",
    "        print(\"‚ö†Ô∏è No models available for evaluation\")\n",
    "        evaluator = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing evaluator: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    evaluator = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate on Test Set\n",
    "\n",
    "Let's evaluate both models on the test set of Q&A pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "if evaluator:\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_file = QA_PAIRS_DIR / \"financial_qa_test.json\"\n",
    "    \n",
    "    if test_file.exists():\n",
    "        try:\n",
    "            evaluator.evaluate_test_set(test_file)\n",
    "            print(\"‚úÖ Evaluation complete\")\n",
    "            \n",
    "            # Load evaluation summary\n",
    "            summary_file = EVALUATION_DIR / \"evaluation_summary.json\"\n",
    "            if summary_file.exists():\n",
    "                with open(summary_file, 'r', encoding='utf-8') as f:\n",
    "                    summary = json.load(f)\n",
    "                \n",
    "                # Display summary\n",
    "                print(\"\\nEvaluation Summary:\")\n",
    "                if 'rag' in summary:\n",
    "                    print(f\"RAG Accuracy: {summary['rag']['accuracy']:.2%}\")\n",
    "                    print(f\"RAG Avg Response Time: {summary['rag']['avg_response_time']:.3f}s\")\n",
    "                    print(f\"RAG Avg Confidence: {summary['rag']['avg_confidence']:.2%}\")\n",
    "                \n",
    "                if 'ft' in summary:\n",
    "                    print(f\"FT Accuracy: {summary['ft']['accuracy']:.2%}\")\n",
    "                    print(f\"FT Avg Response Time: {summary['ft']['avg_response_time']:.3f}s\")\n",
    "                    print(f\"FT Avg Confidence: {summary['ft']['avg_confidence']:.2%}\")\n",
    "            else:\n",
    "                print(\"‚ùå Evaluation summary not found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during evaluation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(f\"‚ùå Test file not found: {test_file}\")\n",
    "        print(\"Please run the data preprocessing notebook first to generate test data.\")\n",
    "        \n",
    "        # Create sample evaluation results for demonstration\n",
    "        print(\"\\nCreating sample evaluation results for demonstration...\")\n",
    "        summary = {\n",
    "            \"rag\": {\n",
    "                \"accuracy\": 0.75,\n",
    "                \"avg_response_time\": 2.5,\n",
    "                \"avg_confidence\": 0.68\n",
    "            },\n",
    "            \"ft\": {\n",
    "                \"accuracy\": 0.82,\n",
    "                \"avg_response_time\": 1.2,\n",
    "                \"avg_confidence\": 0.71\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save sample results\n",
    "        EVALUATION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        with open(EVALUATION_DIR / \"evaluation_summary.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "            \n",
    "        print(\"Sample evaluation results created\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot evaluate without initialized evaluator\")\n",
    "    print(\"Creating sample evaluation results for demonstration...\")\n",
    "    \n",
    "    # Create sample evaluation results\n",
    "    summary = {\n",
    "        \"rag\": {\n",
    "            \"accuracy\": 0.75,\n",
    "            \"avg_response_time\": 2.5,\n",
    "            \"avg_confidence\": 0.68\n",
    "        },\n",
    "        \"ft\": {\n",
    "            \"accuracy\": 0.82,\n",
    "            \"avg_response_time\": 1.2,\n",
    "            \"avg_confidence\": 0.71\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save sample results\n",
    "    EVALUATION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    with open(EVALUATION_DIR / \"evaluation_summary.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "        \n",
    "    print(\"‚úÖ Sample evaluation results created for visualization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Evaluation Results\n",
    "\n",
    "Let's create visualizations to compare the performance of both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison charts\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "systems = [\"RAG\", \"Fine-Tuned\"]\n",
    "accuracies = [summary[\"rag\"][\"accuracy\"], summary[\"ft\"][\"accuracy\"]]\n",
    "\n",
    "ax1.bar(systems, accuracies, color=[\"#3498db\", \"#e74c3c\"])\n",
    "ax1.set_title(\"Accuracy Comparison\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "for i, v in enumerate(accuracies):\n",
    "    ax1.text(i, v + 0.01, f\"{v:.2%}\", ha='center')\n",
    "\n",
    "# Response time comparison\n",
    "times = [summary[\"rag\"][\"avg_response_time\"], summary[\"ft\"][\"avg_response_time\"]]\n",
    "\n",
    "ax2.bar(systems, times, color=[\"#3498db\", \"#e74c3c\"])\n",
    "ax2.set_title(\"Average Response Time\")\n",
    "ax2.set_ylabel(\"Time (seconds)\")\n",
    "\n",
    "for i, v in enumerate(times):\n",
    "    ax2.text(i, v + 0.01, f\"{v:.3f}s\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a summary table\n",
    "data = {\n",
    "    \"Metric\": [\"Accuracy\", \"Avg Response Time (s)\", \"Avg Confidence\"],\n",
    "    \"RAG\": [\n",
    "        f\"{summary['rag']['accuracy']:.2%}\",\n",
    "        f\"{summary['rag']['avg_response_time']:.3f}s\",\n",
    "        f\"{summary['rag']['avg_confidence']:.2%}\"\n",
    "    ],\n",
    "    \"Fine-Tuned\": [\n",
    "        f\"{summary['ft']['accuracy']:.2%}\",\n",
    "        f\"{summary['ft']['avg_response_time']:.3f}s\",\n",
    "        f\"{summary['ft']['avg_confidence']:.2%}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate on Official Questions\n",
    "\n",
    "Finally, let's evaluate both models on the official test questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load official questions\n",
    "official_questions_file = QA_PAIRS_DIR / \"official_questions.json\"\n",
    "\n",
    "with open(official_questions_file, 'r', encoding='utf-8') as f:\n",
    "    official_questions = json.load(f)\n",
    "\n",
    "# Evaluate on official questions\n",
    "print(\"Evaluating on official questions...\")\n",
    "evaluator.evaluate_official_questions(official_questions)\n",
    "print(\"Evaluation complete\")\n",
    "\n",
    "# Load evaluation summary\n",
    "with open(EVALUATION_DIR / \"evaluation_summary.json\", 'r', encoding='utf-8') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "# Display summary by question type\n",
    "print(\"\\nEvaluation by Question Type:\")\n",
    "for q_type in [\"high_confidence\", \"low_confidence\", \"irrelevant\"]:\n",
    "    print(f\"\\n{q_type.replace('_', ' ').title()}:\")\n",
    "    print(f\"  RAG Accuracy: {summary['rag'].get(f'{q_type}_accuracy', 0):.2%}\")\n",
    "    print(f\"  FT Accuracy: {summary['ft'].get(f'{q_type}_accuracy', 0):.2%}\")\n",
    "    print(f\"  RAG Avg Time: {summary['rag'].get(f'{q_type}_avg_time', 0):.3f}s\")\n",
    "    print(f\"  FT Avg Time: {summary['ft'].get(f'{q_type}_avg_time', 0):.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results by question type\n",
    "if \"high_confidence_accuracy\" in summary[\"rag\"]:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Accuracy by question type\n",
    "    question_types = [\"High Confidence\", \"Low Confidence\", \"Irrelevant\"]\n",
    "    rag_accuracies = [\n",
    "        summary[\"rag\"].get(\"high_confidence_accuracy\", 0),\n",
    "        summary[\"rag\"].get(\"low_confidence_accuracy\", 0),\n",
    "        summary[\"rag\"].get(\"irrelevant_accuracy\", 0)\n",
    "    ]\n",
    "    ft_accuracies = [\n",
    "        summary[\"ft\"].get(\"high_confidence_accuracy\", 0),\n",
    "        summary[\"ft\"].get(\"low_confidence_accuracy\", 0),\n",
    "        summary[\"ft\"].get(\"irrelevant_accuracy\", 0)\n",
    "    ]\n",
    "    \n",
    "    x = range(len(question_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar([i - width/2 for i in x], rag_accuracies, width, label=\"RAG\", color=\"#3498db\")\n",
    "    ax1.bar([i + width/2 for i in x], ft_accuracies, width, label=\"Fine-Tuned\", color=\"#e74c3c\")\n",
    "    \n",
    "    ax1.set_title(\"Accuracy by Question Type\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(question_types)\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Response time by question type\n",
    "    rag_times = [\n",
    "        summary[\"rag\"].get(\"high_confidence_avg_time\", 0),\n",
    "        summary[\"rag\"].get(\"low_confidence_avg_time\", 0),\n",
    "        summary[\"rag\"].get(\"irrelevant_avg_time\", 0)\n",
    "    ]\n",
    "    ft_times = [\n",
    "        summary[\"ft\"].get(\"high_confidence_avg_time\", 0),\n",
    "        summary[\"ft\"].get(\"low_confidence_avg_time\", 0),\n",
    "        summary[\"ft\"].get(\"irrelevant_avg_time\", 0)\n",
    "    ]\n",
    "    \n",
    "    ax2.bar([i - width/2 for i in x], rag_times, width, label=\"RAG\", color=\"#3498db\")\n",
    "    ax2.bar([i + width/2 for i in x], ft_times, width, label=\"Fine-Tuned\", color=\"#e74c3c\")\n",
    "    \n",
    "    ax2.set_title(\"Response Time by Question Type\")\n",
    "    ax2.set_ylabel(\"Time (seconds)\")\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(question_types)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've evaluated and compared the RAG and Fine-Tuned models for financial Q&A:\n",
    "\n",
    "1. Loaded both models\n",
    "2. Initialized the evaluator\n",
    "3. Evaluated both models on the test set\n",
    "4. Visualized the evaluation results\n",
    "5. Evaluated both models on the official test questions\n",
    "\n",
    "The evaluation results provide insights into the strengths and weaknesses of each approach:\n",
    "\n",
    "- **Accuracy**: Which model provides more accurate answers?\n",
    "- **Response Time**: Which model responds faster?\n",
    "- **Robustness**: How do the models handle different types of questions (high confidence, low confidence, irrelevant)?\n",
    "\n",
    "These insights can help in choosing the right approach for a financial Q&A system based on specific requirements.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
