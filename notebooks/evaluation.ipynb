{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Financial Q&A Systems - Evaluation\n",
        "\n",
        "This notebook compares the performance of the RAG and Fine-Tuned models for financial Q&A.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add the project root to the path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "# Import project modules\n",
        "from src.rag_system.rag_system import RAGSystem\n",
        "from src.fine_tuning.ft_model import FineTunedModel\n",
        "from src.evaluation.evaluator import Evaluator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "DATA_DIR = project_root / \"data\"\n",
        "QA_PAIRS_DIR = DATA_DIR / \"qa_pairs\"\n",
        "RAG_MODEL_DIR = project_root / \"models\" / \"rag\"\n",
        "FT_MODEL_DIR = project_root / \"models\" / \"fine_tuned\"\n",
        "EVALUATION_DIR = project_root / \"evaluation_results\"\n",
        "\n",
        "# Create evaluation directory if it doesn't exist\n",
        "EVALUATION_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Models\n",
        "\n",
        "First, let's load both the RAG and Fine-Tuned models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the RAG system\n",
        "print(\"Loading RAG system...\")\n",
        "rag_system = RAGSystem()\n",
        "rag_system.load(RAG_MODEL_DIR)\n",
        "print(\"RAG system loaded\")\n",
        "\n",
        "# Load the Fine-Tuned model\n",
        "print(\"\\nLoading Fine-Tuned model...\")\n",
        "ft_model = FineTunedModel(model_path=FT_MODEL_DIR)\n",
        "print(\"Fine-Tuned model loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Initialize the Evaluator\n",
        "\n",
        "Now, let's initialize the evaluator with both models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the evaluator\n",
        "print(\"Initializing evaluator...\")\n",
        "evaluator = Evaluator(\n",
        "    rag_system=rag_system,\n",
        "    ft_model=ft_model,\n",
        "    output_dir=EVALUATION_DIR\n",
        ")\n",
        "print(\"Evaluator initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Evaluate on Test Set\n",
        "\n",
        "Let's evaluate both models on the test set of Q&A pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "test_file = QA_PAIRS_DIR / \"financial_qa_test.json\"\n",
        "evaluator.evaluate_test_set(test_file)\n",
        "print(\"Evaluation complete\")\n",
        "\n",
        "# Load evaluation summary\n",
        "with open(EVALUATION_DIR / \"evaluation_summary.json\", 'r', encoding='utf-8') as f:\n",
        "    summary = json.load(f)\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nEvaluation Summary:\")\n",
        "print(f\"RAG Accuracy: {summary['rag']['accuracy']:.2%}\")\n",
        "print(f\"FT Accuracy: {summary['ft']['accuracy']:.2%}\")\n",
        "print(f\"RAG Avg Response Time: {summary['rag']['avg_response_time']:.3f}s\")\n",
        "print(f\"FT Avg Response Time: {summary['ft']['avg_response_time']:.3f}s\")\n",
        "print(f\"RAG Avg Confidence: {summary['rag']['avg_confidence']:.2%}\")\n",
        "print(f\"FT Avg Confidence: {summary['ft']['avg_confidence']:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Visualize Evaluation Results\n",
        "\n",
        "Let's create visualizations to compare the performance of both models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison charts\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "systems = [\"RAG\", \"Fine-Tuned\"]\n",
        "accuracies = [summary[\"rag\"][\"accuracy\"], summary[\"ft\"][\"accuracy\"]]\n",
        "\n",
        "ax1.bar(systems, accuracies, color=[\"#3498db\", \"#e74c3c\"])\n",
        "ax1.set_title(\"Accuracy Comparison\")\n",
        "ax1.set_ylabel(\"Accuracy\")\n",
        "ax1.set_ylim(0, 1)\n",
        "\n",
        "for i, v in enumerate(accuracies):\n",
        "    ax1.text(i, v + 0.01, f\"{v:.2%}\", ha='center')\n",
        "\n",
        "# Response time comparison\n",
        "times = [summary[\"rag\"][\"avg_response_time\"], summary[\"ft\"][\"avg_response_time\"]]\n",
        "\n",
        "ax2.bar(systems, times, color=[\"#3498db\", \"#e74c3c\"])\n",
        "ax2.set_title(\"Average Response Time\")\n",
        "ax2.set_ylabel(\"Time (seconds)\")\n",
        "\n",
        "for i, v in enumerate(times):\n",
        "    ax2.text(i, v + 0.01, f\"{v:.3f}s\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a summary table\n",
        "data = {\n",
        "    \"Metric\": [\"Accuracy\", \"Avg Response Time (s)\", \"Avg Confidence\"],\n",
        "    \"RAG\": [\n",
        "        f\"{summary['rag']['accuracy']:.2%}\",\n",
        "        f\"{summary['rag']['avg_response_time']:.3f}s\",\n",
        "        f\"{summary['rag']['avg_confidence']:.2%}\"\n",
        "    ],\n",
        "    \"Fine-Tuned\": [\n",
        "        f\"{summary['ft']['accuracy']:.2%}\",\n",
        "        f\"{summary['ft']['avg_response_time']:.3f}s\",\n",
        "        f\"{summary['ft']['avg_confidence']:.2%}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Evaluate on Official Questions\n",
        "\n",
        "Finally, let's evaluate both models on the official test questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load official questions\n",
        "official_questions_file = QA_PAIRS_DIR / \"official_questions.json\"\n",
        "\n",
        "with open(official_questions_file, 'r', encoding='utf-8') as f:\n",
        "    official_questions = json.load(f)\n",
        "\n",
        "# Evaluate on official questions\n",
        "print(\"Evaluating on official questions...\")\n",
        "evaluator.evaluate_official_questions(official_questions)\n",
        "print(\"Evaluation complete\")\n",
        "\n",
        "# Load evaluation summary\n",
        "with open(EVALUATION_DIR / \"evaluation_summary.json\", 'r', encoding='utf-8') as f:\n",
        "    summary = json.load(f)\n",
        "\n",
        "# Display summary by question type\n",
        "print(\"\\nEvaluation by Question Type:\")\n",
        "for q_type in [\"high_confidence\", \"low_confidence\", \"irrelevant\"]:\n",
        "    print(f\"\\n{q_type.replace('_', ' ').title()}:\")\n",
        "    print(f\"  RAG Accuracy: {summary['rag'].get(f'{q_type}_accuracy', 0):.2%}\")\n",
        "    print(f\"  FT Accuracy: {summary['ft'].get(f'{q_type}_accuracy', 0):.2%}\")\n",
        "    print(f\"  RAG Avg Time: {summary['rag'].get(f'{q_type}_avg_time', 0):.3f}s\")\n",
        "    print(f\"  FT Avg Time: {summary['ft'].get(f'{q_type}_avg_time', 0):.3f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results by question type\n",
        "if \"high_confidence_accuracy\" in summary[\"rag\"]:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # Accuracy by question type\n",
        "    question_types = [\"High Confidence\", \"Low Confidence\", \"Irrelevant\"]\n",
        "    rag_accuracies = [\n",
        "        summary[\"rag\"].get(\"high_confidence_accuracy\", 0),\n",
        "        summary[\"rag\"].get(\"low_confidence_accuracy\", 0),\n",
        "        summary[\"rag\"].get(\"irrelevant_accuracy\", 0)\n",
        "    ]\n",
        "    ft_accuracies = [\n",
        "        summary[\"ft\"].get(\"high_confidence_accuracy\", 0),\n",
        "        summary[\"ft\"].get(\"low_confidence_accuracy\", 0),\n",
        "        summary[\"ft\"].get(\"irrelevant_accuracy\", 0)\n",
        "    ]\n",
        "    \n",
        "    x = range(len(question_types))\n",
        "    width = 0.35\n",
        "    \n",
        "    ax1.bar([i - width/2 for i in x], rag_accuracies, width, label=\"RAG\", color=\"#3498db\")\n",
        "    ax1.bar([i + width/2 for i in x], ft_accuracies, width, label=\"Fine-Tuned\", color=\"#e74c3c\")\n",
        "    \n",
        "    ax1.set_title(\"Accuracy by Question Type\")\n",
        "    ax1.set_ylabel(\"Accuracy\")\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(question_types)\n",
        "    ax1.legend()\n",
        "    ax1.set_ylim(0, 1)\n",
        "    \n",
        "    # Response time by question type\n",
        "    rag_times = [\n",
        "        summary[\"rag\"].get(\"high_confidence_avg_time\", 0),\n",
        "        summary[\"rag\"].get(\"low_confidence_avg_time\", 0),\n",
        "        summary[\"rag\"].get(\"irrelevant_avg_time\", 0)\n",
        "    ]\n",
        "    ft_times = [\n",
        "        summary[\"ft\"].get(\"high_confidence_avg_time\", 0),\n",
        "        summary[\"ft\"].get(\"low_confidence_avg_time\", 0),\n",
        "        summary[\"ft\"].get(\"irrelevant_avg_time\", 0)\n",
        "    ]\n",
        "    \n",
        "    ax2.bar([i - width/2 for i in x], rag_times, width, label=\"RAG\", color=\"#3498db\")\n",
        "    ax2.bar([i + width/2 for i in x], ft_times, width, label=\"Fine-Tuned\", color=\"#e74c3c\")\n",
        "    \n",
        "    ax2.set_title(\"Response Time by Question Type\")\n",
        "    ax2.set_ylabel(\"Time (seconds)\")\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(question_types)\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we've evaluated and compared the RAG and Fine-Tuned models for financial Q&A:\n",
        "\n",
        "1. Loaded both models\n",
        "2. Initialized the evaluator\n",
        "3. Evaluated both models on the test set\n",
        "4. Visualized the evaluation results\n",
        "5. Evaluated both models on the official test questions\n",
        "\n",
        "The evaluation results provide insights into the strengths and weaknesses of each approach:\n",
        "\n",
        "- **Accuracy**: Which model provides more accurate answers?\n",
        "- **Response Time**: Which model responds faster?\n",
        "- **Robustness**: How do the models handle different types of questions (high confidence, low confidence, irrelevant)?\n",
        "\n",
        "These insights can help in choosing the right approach for a financial Q&A system based on specific requirements.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
