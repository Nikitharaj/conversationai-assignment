{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Financial Q&A Systems - Data Preprocessing\n",
        "\n",
        "This notebook demonstrates the process of preprocessing financial documents and generating Q&A pairs for the RAG and Fine-Tuned systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add the project root to the path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "# Import project modules\n",
        "from src.data_processing.document_processor import DocumentProcessor\n",
        "from src.data_processing.qa_generator import QAGenerator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "DATA_DIR = project_root / \"data\"\n",
        "RAW_DIR = DATA_DIR / \"raw\"\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "QA_PAIRS_DIR = DATA_DIR / \"qa_pairs\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "QA_PAIRS_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Process Financial Documents\n",
        "\n",
        "In this step, we'll process the raw financial documents (PDF, Excel, HTML) and convert them to clean text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the document processor\n",
        "processor = DocumentProcessor(output_dir=PROCESSED_DIR)\n",
        "\n",
        "# List files in the raw directory\n",
        "raw_files = list(RAW_DIR.glob(\"*\"))\n",
        "print(f\"Found {len(raw_files)} files in the raw directory:\")\n",
        "for file in raw_files:\n",
        "    print(f\"  - {file.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process each file\n",
        "processed_files = []\n",
        "\n",
        "for file_path in raw_files:\n",
        "    try:\n",
        "        print(f\"Processing {file_path.name}...\")\n",
        "        processed_text = processor.process_document(file_path)\n",
        "        processed_files.append(PROCESSED_DIR / f\"{file_path.stem}.txt\")\n",
        "        print(f\"  - Processed text length: {len(processed_text)} characters\")\n",
        "    except Exception as e:\n",
        "        print(f\"  - Error processing {file_path.name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Segment Documents by Section\n",
        "\n",
        "Now, we'll segment the processed documents into sections (income statement, balance sheet, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment each processed document\n",
        "segmented_documents = {}\n",
        "\n",
        "for file_path in processed_files:\n",
        "    try:\n",
        "        print(f\"Segmenting {file_path.name}...\")\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        \n",
        "        # Segment the document\n",
        "        sections = processor.segment_by_section(text)\n",
        "        segmented_documents[file_path.stem] = sections\n",
        "        \n",
        "        print(f\"  - Found {len(sections)} sections:\")\n",
        "        for section_name in sections.keys():\n",
        "            print(f\"    - {section_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  - Error segmenting {file_path.name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Generate Q&A Pairs\n",
        "\n",
        "Now, we'll generate Q&A pairs from the processed documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Q&A generator\n",
        "qa_generator = QAGenerator(output_dir=QA_PAIRS_DIR)\n",
        "\n",
        "# Generate Q&A pairs for each processed document\n",
        "all_qa_pairs = []\n",
        "\n",
        "for file_path in processed_files:\n",
        "    try:\n",
        "        print(f\"Generating Q&A pairs from {file_path.name}...\")\n",
        "        qa_pairs = qa_generator.generate_qa_pairs(file_path, num_pairs=10)\n",
        "        all_qa_pairs.extend(qa_pairs)\n",
        "        print(f\"  - Generated {len(qa_pairs)} Q&A pairs\")\n",
        "    except Exception as e:\n",
        "        print(f\"  - Error generating Q&A pairs from {file_path.name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Split Q&A Pairs into Train and Test Sets\n",
        "\n",
        "We'll split the Q&A pairs into training (40) and testing (10) sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure we have at least 50 Q&A pairs\n",
        "print(f\"Total Q&A pairs generated: {len(all_qa_pairs)}\")\n",
        "\n",
        "if len(all_qa_pairs) < 50:\n",
        "    print(\"Warning: Less than 50 Q&A pairs generated. Need to generate more.\")\n",
        "else:\n",
        "    # Split into train and test sets\n",
        "    train_pairs, test_pairs = qa_generator.split_train_test(\n",
        "        all_qa_pairs,\n",
        "        test_ratio=10/50  # 10 test, 40 train\n",
        "    )\n",
        "    \n",
        "    print(f\"Training set size: {len(train_pairs)}\")\n",
        "    print(f\"Testing set size: {len(test_pairs)}\")\n",
        "    \n",
        "    # Save the split\n",
        "    qa_generator.save_train_test_split(train_pairs, test_pairs, \"financial_qa\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Analyze Q&A Pairs\n",
        "\n",
        "Let's analyze the generated Q&A pairs to understand their characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the train and test sets\n",
        "with open(QA_PAIRS_DIR / \"financial_qa_train.json\", 'r', encoding='utf-8') as f:\n",
        "    train_pairs = json.load(f)\n",
        "\n",
        "with open(QA_PAIRS_DIR / \"financial_qa_test.json\", 'r', encoding='utf-8') as f:\n",
        "    test_pairs = json.load(f)\n",
        "\n",
        "# Analyze question and answer lengths\n",
        "train_q_lengths = [len(pair[\"question\"]) for pair in train_pairs]\n",
        "train_a_lengths = [len(pair[\"answer\"]) for pair in train_pairs]\n",
        "\n",
        "test_q_lengths = [len(pair[\"question\"]) for pair in test_pairs]\n",
        "test_a_lengths = [len(pair[\"answer\"]) for pair in test_pairs]\n",
        "\n",
        "# Plot the distributions\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Question length distribution\n",
        "sns.histplot(train_q_lengths, kde=True, ax=ax1, label=\"Train\")\n",
        "sns.histplot(test_q_lengths, kde=True, ax=ax1, label=\"Test\")\n",
        "ax1.set_title(\"Question Length Distribution\")\n",
        "ax1.set_xlabel(\"Length (characters)\")\n",
        "ax1.legend()\n",
        "\n",
        "# Answer length distribution\n",
        "sns.histplot(train_a_lengths, kde=True, ax=ax2, label=\"Train\")\n",
        "sns.histplot(test_a_lengths, kde=True, ax=ax2, label=\"Test\")\n",
        "ax2.set_title(\"Answer Length Distribution\")\n",
        "ax2.set_xlabel(\"Length (characters)\")\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"Question Length Statistics:\")\n",
        "print(f\"  - Train: Min={min(train_q_lengths)}, Max={max(train_q_lengths)}, Avg={sum(train_q_lengths)/len(train_q_lengths):.1f}\")\n",
        "print(f\"  - Test: Min={min(test_q_lengths)}, Max={max(test_q_lengths)}, Avg={sum(test_q_lengths)/len(test_q_lengths):.1f}\")\n",
        "\n",
        "print(\"\\nAnswer Length Statistics:\")\n",
        "print(f\"  - Train: Min={min(train_a_lengths)}, Max={max(train_a_lengths)}, Avg={sum(train_a_lengths)/len(train_a_lengths):.1f}\")\n",
        "print(f\"  - Test: Min={min(test_a_lengths)}, Max={max(test_a_lengths)}, Avg={sum(test_a_lengths)/len(test_a_lengths):.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Create Official Test Questions\n",
        "\n",
        "Finally, let's create the three official test questions: high-confidence, low-confidence, and irrelevant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create official test questions\n",
        "official_questions = [\n",
        "    {\n",
        "        \"question\": \"What was the revenue in the most recent fiscal year?\",\n",
        "        \"answer\": \"The revenue for the most recent fiscal year was $X million.\",  # Replace X with actual value\n",
        "        \"type\": \"high_confidence\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does the company's profit margin compare to industry average?\",\n",
        "        \"answer\": \"The company's profit margin was X% compared to the industry average of Y%.\",  # Replace X, Y with actual values\n",
        "        \"type\": \"low_confidence\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is your favorite color?\",\n",
        "        \"answer\": \"I can only answer questions related to financial information in the provided documents.\",\n",
        "        \"type\": \"irrelevant\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Save the official questions\n",
        "with open(QA_PAIRS_DIR / \"official_questions.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump(official_questions, f, indent=2)\n",
        "\n",
        "print(\"Official test questions created and saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we've completed the following steps:\n",
        "\n",
        "1. Processed raw financial documents into clean text\n",
        "2. Segmented documents by section\n",
        "3. Generated Q&A pairs from the processed documents\n",
        "4. Split the Q&A pairs into training and testing sets\n",
        "5. Analyzed the characteristics of the Q&A pairs\n",
        "6. Created official test questions for evaluation\n",
        "\n",
        "The processed data is now ready for use in the RAG and Fine-Tuned systems.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
