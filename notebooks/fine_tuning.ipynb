{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Q&A Systems - Fine-Tuned Model Implementation\n",
    "\n",
    "This notebook demonstrates the implementation of the Fine-Tuned model for financial Q&A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src.fine_tuning.fine_tuner import FineTuner\n",
    "from src.fine_tuning.ft_model import FineTunedModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = project_root / \"data\"\n",
    "QA_PAIRS_DIR = DATA_DIR / \"qa_pairs\"\n",
    "FT_MODEL_DIR = project_root / \"models\" / \"fine_tuned\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "QA_PAIRS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FT_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Analyze Q&A Pairs\n",
    "\n",
    "First, let's load and analyze the Q&A pairs that we'll use for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test sets\n",
    "train_file = QA_PAIRS_DIR / \"financial_qa_train.json\"\n",
    "test_file = QA_PAIRS_DIR / \"financial_qa_test.json\"\n",
    "\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    train_pairs = json.load(f)\n",
    "\n",
    "with open(test_file, 'r', encoding='utf-8') as f:\n",
    "    test_pairs = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(train_pairs)} training pairs and {len(test_pairs)} testing pairs\")\n",
    "\n",
    "# Display a few examples\n",
    "print(\"\\nTraining Examples:\")\n",
    "for i in range(min(3, len(train_pairs))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Q: {train_pairs[i]['question']}\")\n",
    "    print(f\"A: {train_pairs[i]['answer']}\")\n",
    "\n",
    "print(\"\\nTesting Examples:\")\n",
    "for i in range(min(3, len(test_pairs))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Q: {test_pairs[i]['question']}\")\n",
    "    print(f\"A: {test_pairs[i]['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Fine-Tuner\n",
    "\n",
    "Now, let's initialize the fine-tuner with a small language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the fine-tuner\n",
    "print(\"Initializing fine-tuner...\")\n",
    "fine_tuner = FineTuner(\n",
    "    model_name=\"distilgpt2\",\n",
    "    output_dir=FT_MODEL_DIR,\n",
    "    use_peft=True  # Use Parameter-Efficient Fine-Tuning (LoRA)\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "fine_tuner.max_length = 512\n",
    "fine_tuner.batch_size = 8 if device == \"cuda\" else 4\n",
    "fine_tuner.learning_rate = 5e-5\n",
    "fine_tuner.num_epochs = 3\n",
    "\n",
    "print(\"Fine-tuner initialized with the following parameters:\")\n",
    "print(f\"  - Model: {fine_tuner.model_name}\")\n",
    "print(f\"  - PEFT: {fine_tuner.use_peft}\")\n",
    "print(f\"  - Max length: {fine_tuner.max_length}\")\n",
    "print(f\"  - Batch size: {fine_tuner.batch_size}\")\n",
    "print(f\"  - Learning rate: {fine_tuner.learning_rate}\")\n",
    "print(f\"  - Epochs: {fine_tuner.num_epochs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Pre-Training Evaluation\n",
    "\n",
    "Before fine-tuning, let's evaluate the base model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the base model\n",
    "print(\"Evaluating base model...\")\n",
    "try:\n",
    "    pre_training_metrics = fine_tuner.evaluate(test_file)\n",
    "    \n",
    "    print(\"\\nPre-training evaluation results:\")\n",
    "    print(f\"Accuracy: {pre_training_metrics['accuracy']:.2%}\")\n",
    "    print(f\"Average response time: {pre_training_metrics['avg_response_time']:.3f}s\")\n",
    "    \n",
    "    # Display a few examples\n",
    "    print(\"\\nExample predictions:\")\n",
    "    for i, result in enumerate(pre_training_metrics['results'][:3]):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Q: {result['question']}\")\n",
    "        print(f\"Ground truth: {result['ground_truth']}\")\n",
    "        print(f\"Generated: {result['generated']}\")\n",
    "        print(f\"Correct: {result['is_correct']}\")\n",
    "        print(f\"Similarity: {result['similarity']:.2f}\")\n",
    "        print(f\"Response time: {result['response_time']:.3f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in pre-training evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fine-Tune the Model\n",
    "\n",
    "Now, let's fine-tune the model on our training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "print(\"Starting fine-tuning...\")\n",
    "try:\n",
    "    fine_tuner.fine_tune(train_file)\n",
    "    \n",
    "    print(\"\\nFine-tuning complete!\")\n",
    "    print(f\"Model saved to {fine_tuner.output_dir}\")\n",
    "    \n",
    "    # Check if training metrics file was created\n",
    "    metrics_file = FT_MODEL_DIR / \"training_metrics.json\"\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            training_metrics = json.load(f)\n",
    "        \n",
    "        print(\"\\nTraining metrics:\")\n",
    "        print(f\"Training time: {training_metrics['train_runtime']:.2f} seconds\")\n",
    "        print(f\"Samples per second: {training_metrics['train_samples_per_second']:.2f}\")\n",
    "        print(f\"Final loss: {training_metrics['train_loss']:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in fine-tuning: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Post-Training Evaluation\n",
    "\n",
    "After fine-tuning, let's evaluate the model again on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "print(\"Loading fine-tuned model...\")\n",
    "try:\n",
    "    ft_model = FineTunedModel(model_path=FT_MODEL_DIR)\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"What was the revenue in 2023?\",\n",
    "        \"How much profit did the company make?\",\n",
    "        \"What are the total assets?\"\n",
    "    ]\n",
    "    \n",
    "    # Process each query\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nProcessing query: {query}\")\n",
    "        result = ft_model.process_query(query)\n",
    "        \n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(f\"Response time: {result['response_time']:.3f}s\")\n",
    "        if 'confidence' in result:\n",
    "            print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing fine-tuned model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare Pre-Training and Post-Training Results\n",
    "\n",
    "Let's compare the model's performance before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics\n",
    "metrics = {\n",
    "    \"Accuracy\": [pre_training_metrics[\"accuracy\"], accuracy],\n",
    "    \"Avg Response Time (s)\": [pre_training_metrics[\"avg_response_time\"], avg_response_time]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(metrics, index=[\"Pre-Training\", \"Post-Training\"])\n",
    "print(df)\n",
    "\n",
    "# Create comparison charts\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.bar([\"Pre-Training\", \"Post-Training\"], [pre_training_metrics[\"accuracy\"], accuracy], color=[\"#3498db\", \"#e74c3c\"])\n",
    "ax1.set_title(\"Accuracy Comparison\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "for i, v in enumerate([pre_training_metrics[\"accuracy\"], accuracy]):\n",
    "    ax1.text(i, v + 0.01, f\"{v:.2%}\", ha='center')\n",
    "\n",
    "# Response time comparison\n",
    "ax2.bar([\"Pre-Training\", \"Post-Training\"], [pre_training_metrics[\"avg_response_time\"], avg_response_time], color=[\"#3498db\", \"#e74c3c\"])\n",
    "ax2.set_title(\"Average Response Time\")\n",
    "ax2.set_ylabel(\"Time (seconds)\")\n",
    "\n",
    "for i, v in enumerate([pre_training_metrics[\"avg_response_time\"], avg_response_time]):\n",
    "    ax2.text(i, v + 0.01, f\"{v:.3f}s\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test with Official Questions\n",
    "\n",
    "Finally, let's test the fine-tuned model with the official test questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load official questions\n",
    "official_questions_file = QA_PAIRS_DIR / \"official_questions.json\"\n",
    "\n",
    "with open(official_questions_file, 'r', encoding='utf-8') as f:\n",
    "    official_questions = json.load(f)\n",
    "\n",
    "# Test each official question\n",
    "print(\"Testing official questions:\")\n",
    "for i, q_data in enumerate(official_questions):\n",
    "    question = q_data[\"question\"]\n",
    "    ground_truth = q_data[\"answer\"]\n",
    "    question_type = q_data[\"type\"]\n",
    "    \n",
    "    print(f\"\\nQuestion {i+1} ({question_type}):\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"Ground truth: {ground_truth}\")\n",
    "    \n",
    "    # Process the query\n",
    "    start_time = time.time()\n",
    "    result = ft_model.process_query(question)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Response time: {result['response_time']:.3f}s\")\n",
    "    \n",
    "    if result[\"is_filtered\"]:\n",
    "        print(\"Query was filtered by input-side guardrails\")\n",
    "        \n",
    "    # Calculate similarity for non-filtered results\n",
    "    if not result[\"is_filtered\"]:\n",
    "        from difflib import SequenceMatcher\n",
    "        similarity = SequenceMatcher(None, result[\"answer\"].lower(), ground_truth.lower()).ratio()\n",
    "        is_correct = similarity > 0.5\n",
    "        print(f\"Similarity: {similarity:.2f}\")\n",
    "        print(f\"Correct: {'Yes' if is_correct else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've implemented the Fine-Tuned model for financial Q&A:\n",
    "\n",
    "1. Loaded and analyzed the Q&A pairs\n",
    "2. Initialized the fine-tuner with a small language model\n",
    "3. Evaluated the base model on the test set\n",
    "4. Fine-tuned the model on our training data\n",
    "5. Evaluated the fine-tuned model on the test set\n",
    "6. Compared the model's performance before and after fine-tuning\n",
    "7. Tested the fine-tuned model with official questions\n",
    "\n",
    "The Fine-Tuned model is now ready for evaluation and comparison with the RAG "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
