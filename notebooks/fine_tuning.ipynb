{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Q&A Systems - Fine-Tuned Model Implementation\n",
    "\n",
    "This notebook demonstrates the implementation of the Fine-Tuned model for financial Q&A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src.fine_tuning.fine_tuner import FineTuner\n",
    "\n",
    "print(\"✅ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = project_root / \"data\"\n",
    "QA_PAIRS_DIR = DATA_DIR / \"qa_pairs\"\n",
    "FT_MODEL_DIR = project_root / \"models\" / \"fine_tuned\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "QA_PAIRS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FT_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Analyze Q&A Pairs\n",
    "\n",
    "First, let's load and analyze the Q&A pairs that we'll use for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test sets\n",
    "train_file = QA_PAIRS_DIR / \"financial_qa_train.json\"\n",
    "test_file = QA_PAIRS_DIR / \"financial_qa_test.json\"\n",
    "\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    train_pairs = json.load(f)\n",
    "\n",
    "with open(test_file, 'r', encoding='utf-8') as f:\n",
    "    test_pairs = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(train_pairs)} training pairs and {len(test_pairs)} testing pairs\")\n",
    "\n",
    "# Display a few examples\n",
    "print(\"\\nTraining Examples:\")\n",
    "for i in range(min(3, len(train_pairs))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Q: {train_pairs[i]['question']}\")\n",
    "    print(f\"A: {train_pairs[i]['answer']}\")\n",
    "\n",
    "print(\"\\nTesting Examples:\")\n",
    "for i in range(min(3, len(test_pairs))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Q: {test_pairs[i]['question']}\")\n",
    "    print(f\"A: {test_pairs[i]['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Fine-Tuner\n",
    "\n",
    "Now, let's initialize the fine-tuner with a small language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the fine-tuner\n",
    "print(\"Initializing fine-tuner...\")\n",
    "fine_tuner = FineTuner(\n",
    "    model_name=\"distilgpt2\",\n",
    "    output_dir=FT_MODEL_DIR,\n",
    "    use_peft=True  # Use Parameter-Efficient Fine-Tuning (LoRA)\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "fine_tuner.max_length = 512\n",
    "fine_tuner.batch_size = 8 if device == \"cuda\" else 4\n",
    "fine_tuner.learning_rate = 5e-5\n",
    "fine_tuner.num_epochs = 3\n",
    "\n",
    "print(\"Fine-tuner initialized with the following parameters:\")\n",
    "print(f\"  - Model: {fine_tuner.model_name}\")\n",
    "print(f\"  - PEFT: {fine_tuner.use_peft}\")\n",
    "print(f\"  - Max length: {fine_tuner.max_length}\")\n",
    "print(f\"  - Batch size: {fine_tuner.batch_size}\")\n",
    "print(f\"  - Learning rate: {fine_tuner.learning_rate}\")\n",
    "print(f\"  - Epochs: {fine_tuner.num_epochs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Pre-Training Evaluation\n",
    "\n",
    "Before fine-tuning, let's evaluate the base model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the base model\n",
    "print(\"Evaluating base model...\")\n",
    "try:\n",
    "    pre_training_metrics = fine_tuner.evaluate(test_file)\n",
    "    \n",
    "    print(\"\\nPre-training evaluation results:\")\n",
    "    print(f\"Accuracy: {pre_training_metrics['accuracy']:.2%}\")\n",
    "    print(f\"Average response time: {pre_training_metrics['avg_response_time']:.3f}s\")\n",
    "    \n",
    "    # Display a few examples\n",
    "    print(\"\\nExample predictions:\")\n",
    "    for i, result in enumerate(pre_training_metrics['results'][:3]):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Q: {result['question']}\")\n",
    "        print(f\"Ground truth: {result['ground_truth']}\")\n",
    "        print(f\"Generated: {result['generated']}\")\n",
    "        print(f\"Correct: {result['is_correct']}\")\n",
    "        print(f\"Similarity: {result['similarity']:.2f}\")\n",
    "        print(f\"Response time: {result['response_time']:.3f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in pre-training evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fine-Tune the Model\n",
    "\n",
    "Now, let's fine-tune the model on our training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "print(\"Starting fine-tuning...\")\n",
    "try:\n",
    "    fine_tuner.fine_tune(train_file)\n",
    "    \n",
    "    print(\"\\nFine-tuning complete!\")\n",
    "    print(f\"Model saved to {fine_tuner.output_dir}\")\n",
    "    \n",
    "    # Check if training metrics file was created\n",
    "    metrics_file = FT_MODEL_DIR / \"training_metrics.json\"\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            training_metrics = json.load(f)\n",
    "        \n",
    "        print(\"\\nTraining metrics:\")\n",
    "        print(f\"Training time: {training_metrics['train_runtime']:.2f} seconds\")\n",
    "        print(f\"Samples per second: {training_metrics['train_samples_per_second']:.2f}\")\n",
    "        print(f\"Final loss: {training_metrics['train_loss']:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in fine-tuning: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Post-Training Evaluation\n",
    "\n",
    "After fine-tuning, let's evaluate the model again on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "print(\"Testing the fine-tuned model...\")\n",
    "try:\n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"What was the revenue in 2023?\",\n",
    "        \"How much profit did the company make?\",\n",
    "        \"What are the total assets?\"\n",
    "    ]\n",
    "    \n",
    "    # Process each query with the fine_tuner (which now has the fine-tuned model)\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nProcessing query: {query}\")\n",
    "        result = fine_tuner.process_query(query)\n",
    "        \n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(f\"Response time: {result['response_time']:.3f}s\")\n",
    "        if 'confidence' in result:\n",
    "            print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "        print(f\"Filtered: {result.get('is_filtered', False)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing fine-tuned model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare Pre-Training and Post-Training Results\n",
    "\n",
    "Let's compare the model's performance before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-training evaluation \n",
    "print(\"Evaluating fine-tuned model...\")\n",
    "try:\n",
    "    post_training_metrics = fine_tuner.evaluate(test_file)\n",
    "    \n",
    "    print(\"\\nPost-training evaluation results:\")\n",
    "    print(f\"Accuracy: {post_training_metrics['accuracy']:.2%}\")\n",
    "    print(f\"Average response time: {post_training_metrics['avg_response_time']:.3f}s\")\n",
    "    \n",
    "    # Compare metrics (only if pre-training metrics exist)\n",
    "    if 'pre_training_metrics' in locals():\n",
    "        metrics = {\n",
    "            \"Accuracy\": [pre_training_metrics[\"accuracy\"], post_training_metrics[\"accuracy\"]],\n",
    "            \"Avg Response Time (s)\": [pre_training_metrics[\"avg_response_time\"], post_training_metrics[\"avg_response_time\"]]\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(metrics, index=[\"Pre-Training\", \"Post-Training\"])\n",
    "        print(\"\\nComparison:\")\n",
    "        print(df)\n",
    "\n",
    "        # Create comparison charts\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # Accuracy comparison\n",
    "        accuracies = [pre_training_metrics[\"accuracy\"], post_training_metrics[\"accuracy\"]]\n",
    "        ax1.bar([\"Pre-Training\", \"Post-Training\"], accuracies, color=[\"#3498db\", \"#e74c3c\"])\n",
    "        ax1.set_title(\"Accuracy Comparison\")\n",
    "        ax1.set_ylabel(\"Accuracy\")\n",
    "        ax1.set_ylim(0, 1)\n",
    "\n",
    "        for i, v in enumerate(accuracies):\n",
    "            ax1.text(i, v + 0.01, f\"{v:.2%}\", ha='center')\n",
    "\n",
    "        # Response time comparison\n",
    "        times = [pre_training_metrics[\"avg_response_time\"], post_training_metrics[\"avg_response_time\"]]\n",
    "        ax2.bar([\"Pre-Training\", \"Post-Training\"], times, color=[\"#3498db\", \"#e74c3c\"])\n",
    "        ax2.set_title(\"Average Response Time\")\n",
    "        ax2.set_ylabel(\"Time (seconds)\")\n",
    "\n",
    "        for i, v in enumerate(times):\n",
    "            ax2.text(i, v + 0.01, f\"{v:.3f}s\", ha='center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ No pre-training metrics available for comparison\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in post-training evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test with Official Questions\n",
    "\n",
    "Finally, let's test the fine-tuned model with the official test questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load official questions\n",
    "official_questions_file = QA_PAIRS_DIR / \"official_questions.json\"\n",
    "\n",
    "try:\n",
    "    with open(official_questions_file, 'r', encoding='utf-8') as f:\n",
    "        official_questions = json.load(f)\n",
    "\n",
    "    # Test each official question\n",
    "    print(\"Testing official questions:\")\n",
    "    for i, q_data in enumerate(official_questions):\n",
    "        question = q_data[\"question\"]\n",
    "        ground_truth = q_data[\"answer\"]\n",
    "        question_type = q_data[\"type\"]\n",
    "        \n",
    "        print(f\"\\nQuestion {i+1} ({question_type}):\")\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"Ground truth: {ground_truth}\")\n",
    "        \n",
    "        # Process the query using the fine_tuner\n",
    "        try:\n",
    "            result = fine_tuner.process_query(question)\n",
    "            \n",
    "            print(f\"Answer: {result['answer']}\")\n",
    "            print(f\"Response time: {result['response_time']:.3f}s\")\n",
    "            \n",
    "            if result.get(\"is_filtered\", False):\n",
    "                print(\"Query was filtered by input-side guardrails\")\n",
    "                \n",
    "            # Calculate similarity for non-filtered results\n",
    "            if not result.get(\"is_filtered\", False):\n",
    "                from difflib import SequenceMatcher\n",
    "                similarity = SequenceMatcher(None, result[\"answer\"].lower(), ground_truth.lower()).ratio()\n",
    "                is_correct = similarity > 0.5\n",
    "                print(f\"Similarity: {similarity:.2f}\")\n",
    "                print(f\"Correct: {'Yes' if is_correct else 'No'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {e}\")\n",
    "            \n",
    "except FileNotFoundError:\n",
    "    print(f\"Official questions file not found at {official_questions_file}\")\n",
    "    print(\"Creating sample official questions...\")\n",
    "    \n",
    "    # Create sample official questions if the file doesn't exist\n",
    "    sample_official_questions = [\n",
    "        {\n",
    "            \"question\": \"What was the revenue in the most recent fiscal year?\",\n",
    "            \"answer\": \"The revenue for the most recent fiscal year was $1,250 million.\",\n",
    "            \"type\": \"high_confidence\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How does the company's profit margin compare to industry average?\", \n",
    "            \"answer\": \"The company's profit margin was 15% in 2023, which is an improvement from 14.4% in 2022. No specific industry average is provided in the documents.\",\n",
    "            \"type\": \"low_confidence\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is your favorite color?\",\n",
    "            \"answer\": \"I can only answer questions related to financial information in the provided documents.\",\n",
    "            \"type\": \"irrelevant\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Save and test the sample questions\n",
    "    with open(official_questions_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sample_official_questions, f, indent=2)\n",
    "    \n",
    "    print(\"Sample questions created. Rerun this cell to test them.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've implemented the Fine-Tuned model for financial Q&A using the current codebase:\n",
    "\n",
    "### Key Updates Made:\n",
    "- **Unified Model Architecture**: The `FineTuner` class now handles both training and inference, eliminating the need for a separate `FineTunedModel` class\n",
    "- **PEFT Integration**: Parameter-Efficient Fine-Tuning (PEFT) with LoRA is properly integrated for efficient training\n",
    "- **Enhanced Error Handling**: Added robust error handling and fallback mechanisms throughout the pipeline\n",
    "- **Guardrails Implementation**: Input filtering and output validation to handle irrelevant queries and prevent hallucinations\n",
    "\n",
    "### Implementation Features:\n",
    "1. **Data Loading**: Loaded and analyzed Q&A pairs from the financial dataset\n",
    "2. **Model Initialization**: Set up FineTuner with DistilGPT2 and PEFT configuration\n",
    "3. **Training Process**: Fine-tuned the model using the `quick_fine_tune` method for on-the-fly training\n",
    "4. **Testing & Evaluation**: Comprehensive testing with various query types including edge cases\n",
    "5. **Official Questions**: Testing with high-confidence, low-confidence, and irrelevant questions\n",
    "\n",
    "### Technical Specifications:\n",
    "- **Base Model**: DistilGPT2 (lightweight, suitable for resource-constrained environments)\n",
    "- **Fine-Tuning Method**: PEFT with LoRA (efficient parameter updates)\n",
    "- **Training Data**: Financial Q&A pairs from processed documents\n",
    "- **Evaluation Metrics**: Response accuracy, similarity scores, response time, and guardrail effectiveness\n",
    "\n",
    "The Fine-Tuned model is now ready for evaluation and comparison with the RAG system, demonstrating both the power of fine-tuning and the robustness of the current implementation. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
