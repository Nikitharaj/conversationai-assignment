{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Financial Q&A Systems - RAG System Implementation\n",
        "\n",
        "This notebook demonstrates the implementation of the Retrieval-Augmented Generation (RAG) system for financial Q&A.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add the project root to the path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "# Import project modules\n",
        "from src.rag_system.document_chunker import DocumentChunker\n",
        "from src.rag_system.embedding_manager import EmbeddingManager\n",
        "from src.rag_system.answer_generator import AnswerGenerator\n",
        "from src.rag_system.rag_system import RAGSystem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "DATA_DIR = project_root / \"data\"\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "QA_PAIRS_DIR = DATA_DIR / \"qa_pairs\"\n",
        "CHUNKS_DIR = DATA_DIR / \"chunks\"\n",
        "INDEXES_DIR = DATA_DIR / \"indexes\"\n",
        "RAG_MODEL_DIR = project_root / \"models\" / \"rag\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "QA_PAIRS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "INDEXES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RAG_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Document Chunking\n",
        "\n",
        "In this step, we'll chunk the processed documents into smaller segments for retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the document chunker\n",
        "chunker = DocumentChunker(chunk_sizes=[100, 400], chunk_overlap=50)\n",
        "\n",
        "# List processed files\n",
        "processed_files = list(PROCESSED_DIR.glob(\"*.txt\"))\n",
        "print(f\"Found {len(processed_files)} processed files:\")\n",
        "for file in processed_files:\n",
        "    print(f\"  - {file.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chunk each processed file\n",
        "all_chunks = {}\n",
        "\n",
        "for file_path in processed_files:\n",
        "    try:\n",
        "        print(f\"Chunking {file_path.name}...\")\n",
        "        chunks_by_size = chunker.chunk_file(file_path, output_dir=CHUNKS_DIR)\n",
        "        all_chunks[file_path.stem] = chunks_by_size\n",
        "        \n",
        "        # Print statistics for each chunk size\n",
        "        for size, chunks in chunks_by_size.items():\n",
        "            print(f\"  - Size {size}: {len(chunks)} chunks\")\n",
        "    except Exception as e:\n",
        "        print(f\"  - Error chunking {file_path.name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze chunk length distribution\n",
        "chunk_lengths = []\n",
        "for file_name, chunks_by_size in all_chunks.items():\n",
        "    # Use the larger chunk size (400 tokens)\n",
        "    if 400 in chunks_by_size:\n",
        "        for chunk in chunks_by_size[400]:\n",
        "            chunk_lengths.append(len(chunk[\"text\"]))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(chunk_lengths, kde=True)\n",
        "plt.title(\"Chunk Length Distribution (400 token chunks)\")\n",
        "plt.xlabel(\"Length (characters)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Total chunks: {len(chunk_lengths)}\")\n",
        "print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.1f} characters\")\n",
        "print(f\"Min chunk length: {min(chunk_lengths)} characters\")\n",
        "print(f\"Max chunk length: {max(chunk_lengths)} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Embeddings and Indexes\n",
        "\n",
        "Now, we'll create embeddings and indexes for efficient retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the embedding manager\n",
        "embedding_manager = EmbeddingManager(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load chunks (using the larger chunk size for better context)\n",
        "chunk_size = 400\n",
        "all_file_chunks = []\n",
        "\n",
        "# Combine chunks from all files\n",
        "for file_name, chunks_by_size in all_chunks.items():\n",
        "    if chunk_size in chunks_by_size:\n",
        "        # Add file name to each chunk for tracking\n",
        "        for chunk in chunks_by_size[chunk_size]:\n",
        "            chunk[\"file\"] = file_name\n",
        "        \n",
        "        all_file_chunks.extend(chunks_by_size[chunk_size])\n",
        "\n",
        "print(f\"Total chunks to embed: {len(all_file_chunks)}\")\n",
        "\n",
        "# Build indexes\n",
        "print(\"Building indexes...\")\n",
        "start_time = time.time()\n",
        "embedding_manager.build_indexes(all_file_chunks)\n",
        "end_time = time.time()\n",
        "print(f\"Indexes built in {end_time - start_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save indexes for future use\n",
        "print(\"Saving indexes...\")\n",
        "embedding_manager.save_indexes(INDEXES_DIR)\n",
        "print(f\"Indexes saved to {INDEXES_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Test Retrieval Methods\n",
        "\n",
        "Let's test different retrieval methods (dense, sparse, hybrid) to see which performs best.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test queries\n",
        "test_queries = [\n",
        "    \"What was the revenue in the last fiscal year?\",\n",
        "    \"How much profit did the company make?\",\n",
        "    \"What are the total assets?\",\n",
        "    \"What is the debt to equity ratio?\",\n",
        "    \"How did sales compare to the previous year?\"\n",
        "]\n",
        "\n",
        "# Compare retrieval methods\n",
        "retrieval_methods = [\"dense\", \"sparse\", \"hybrid\"]\n",
        "top_k = 3\n",
        "\n",
        "# Function to display retrieval results\n",
        "def display_retrieval_results(query, results):\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for i, result in enumerate(results):\n",
        "        print(f\"Result {i+1} (Score: {result['score']:.4f}, Method: {result['method']}):\")\n",
        "        print(f\"File: {result['chunk'].get('file', 'unknown')}\")\n",
        "        print(f\"Text: {result['chunk']['text'][:200]}...\")\n",
        "        print()\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# Test each query with each retrieval method\n",
        "for query in test_queries:\n",
        "    print(f\"\\nTesting query: {query}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for method in retrieval_methods:\n",
        "        print(f\"\\nMethod: {method}\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        if method == \"dense\":\n",
        "            results = embedding_manager.dense_search(query, top_k=top_k)\n",
        "        elif method == \"sparse\":\n",
        "            results = embedding_manager.sparse_search(query, top_k=top_k, method=\"bm25\")\n",
        "        else:  # hybrid\n",
        "            results = embedding_manager.hybrid_search(query, top_k=top_k)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        print(f\"Retrieved in {(end_time - start_time) * 1000:.2f} ms\")\n",
        "        \n",
        "        for i, result in enumerate(results):\n",
        "            print(f\"Result {i+1} (Score: {result['score']:.4f}, Method: {result['method']}):\")\n",
        "            print(f\"Text: {result['chunk']['text'][:150]}...\")\n",
        "            print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Set Up Answer Generation\n",
        "\n",
        "Now, let's set up the answer generation component using a small language model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the answer generator\n",
        "print(\"Initializing answer generator...\")\n",
        "answer_generator = AnswerGenerator(model_name=\"distilgpt2\")\n",
        "print(\"Answer generator initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test answer generation\n",
        "test_query = \"What was the revenue in the last fiscal year?\"\n",
        "\n",
        "# Get retrieved chunks using hybrid retrieval\n",
        "retrieved_chunks = embedding_manager.hybrid_search(test_query, top_k=3)\n",
        "\n",
        "# Generate answer\n",
        "print(f\"Generating answer for: {test_query}\")\n",
        "answer, confidence, response_time = answer_generator.generate_answer(test_query, retrieved_chunks)\n",
        "\n",
        "print(f\"\\nAnswer: {answer}\")\n",
        "print(f\"Confidence: {confidence:.2f}\")\n",
        "print(f\"Response time: {response_time:.3f}s\")\n",
        "\n",
        "# Apply guardrails\n",
        "modified_answer, is_hallucination = answer_generator.apply_guardrails(test_query, answer, retrieved_chunks)\n",
        "\n",
        "if is_hallucination:\n",
        "    print(\"\\nHallucination detected! Modified answer:\")\n",
        "    print(modified_answer)\n",
        "else:\n",
        "    print(\"\\nNo hallucination detected.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Integrate the RAG System\n",
        "\n",
        "Finally, let's integrate all components into the complete RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the RAG system\n",
        "print(\"Initializing RAG system...\")\n",
        "rag_system = RAGSystem(\n",
        "    embedding_model=\"all-MiniLM-L6-v2\",\n",
        "    llm_model=\"distilgpt2\",\n",
        "    chunk_sizes=[100, 400],\n",
        "    chunk_overlap=50,\n",
        "    retrieval_method=\"hybrid\",\n",
        "    top_k=3\n",
        ")\n",
        "\n",
        "# We'll use the embedding_manager and answer_generator we've already set up\n",
        "rag_system.embedding_manager = embedding_manager\n",
        "rag_system.answer_generator = answer_generator\n",
        "rag_system.is_initialized = True\n",
        "\n",
        "print(\"RAG system initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the RAG system with sample queries\n",
        "test_queries = [\n",
        "    \"What was the revenue in the last fiscal year?\",\n",
        "    \"How much profit did the company make?\",\n",
        "    \"What are the total assets?\",\n",
        "    \"What is your favorite color?\"  # Irrelevant query to test guardrails\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nProcessing query: {query}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    result = rag_system.process_query(query)\n",
        "    \n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "    print(f\"Confidence: {result['confidence']:.2f}\")\n",
        "    print(f\"Response time: {result['response_time']:.3f}s\")\n",
        "    \n",
        "    if result['is_filtered']:\n",
        "        print(\"Query was filtered by input-side guardrails\")\n",
        "    elif result.get('is_hallucination', False):\n",
        "        print(\"Hallucination detected by output-side guardrails\")\n",
        "    \n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save the RAG System\n",
        "\n",
        "Let's save the complete RAG system for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the RAG system\n",
        "print(\"Saving RAG system...\")\n",
        "rag_system.save(RAG_MODEL_DIR)\n",
        "print(f\"RAG system saved to {RAG_MODEL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we've implemented the complete RAG system for financial Q&A:\n",
        "\n",
        "1. Chunked the processed documents into smaller segments\n",
        "2. Created embeddings and indexes for efficient retrieval\n",
        "3. Tested different retrieval methods (dense, sparse, hybrid)\n",
        "4. Set up answer generation with a small language model\n",
        "5. Integrated all components into a complete RAG system\n",
        "6. Saved the system for later use\n",
        "\n",
        "The RAG system is now ready for evaluation and comparison with the Fine-Tuned model.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
