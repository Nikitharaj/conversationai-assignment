{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Q&A Systems - RAG System Implementation\n",
    "\n",
    "This notebook demonstrates the implementation of the Retrieval-Augmented Generation (RAG) system for financial Q&A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CAI/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GGUF_CONFIG_MAPPING' from 'transformers.integrations' (/opt/anaconda3/envs/CAI/lib/python3.10/site-packages/transformers/integrations/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Import project modules\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag_system\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_chunker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocumentChunker\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag_system\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EmbeddingManager\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag_system\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manswer_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AnswerGenerator\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag_system\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag_system\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGSystem\n",
      "File \u001b[0;32m~/Documents/BITS/C_AI_assignment/src/rag_system/embedding_manager.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrank_bm25\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BM25Okapi\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAI/lib/python3.10/site-packages/sentence_transformers/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[1;32m     12\u001b[0m     export_optimized_onnx_model,\n\u001b[1;32m     13\u001b[0m     export_static_quantized_openvino_model,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     CrossEncoder,\n\u001b[1;32m     17\u001b[0m     CrossEncoderModelCardData,\n\u001b[1;32m     18\u001b[0m     CrossEncoderTrainer,\n\u001b[1;32m     19\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAI/lib/python3.10/site-packages/sentence_transformers/backend/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_optimized_onnx_model\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_dynamic_quantized_onnx_model, export_static_quantized_openvino_model\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAI/lib/python3.10/site-packages/sentence_transformers/backend/load.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _save_pretrained_wrapper, backend_should_export, backend_warn_to_save\n\u001b[1;32m     11\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAI/lib/python3.10/site-packages/transformers/configuration_utils.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_gguf_pytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_gguf_checkpoint\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     CONFIG_NAME,\n\u001b[1;32m     30\u001b[0m     PushToHubMixin,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     logging,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_timm_config_dict\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAI/lib/python3.10/site-packages/transformers/modeling_gguf_pytorch_utils.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     GGUF_CONFIG_MAPPING,\n\u001b[1;32m     24\u001b[0m     GGUF_TOKENIZER_MAPPING,\n\u001b[1;32m     25\u001b[0m     _gguf_parse_value,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_torch_available\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_gguf_available\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GGUF_CONFIG_MAPPING' from 'transformers.integrations' (/opt/anaconda3/envs/CAI/lib/python3.10/site-packages/transformers/integrations/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules with enhanced error handling\n",
    "print(\"🔄 Loading RAG system modules...\")\n",
    "imports_successful = True\n",
    "\n",
    "try:\n",
    "    from src.rag_system.document_chunker import DocumentChunker\n",
    "    print(\"✅ DocumentChunker imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ DocumentChunker import failed: {e}\")\n",
    "    imports_successful = False\n",
    "\n",
    "try:\n",
    "    from src.rag_system.embedding_manager import EmbeddingManager\n",
    "    print(\"✅ EmbeddingManager imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ EmbeddingManager import failed: {e}\")\n",
    "    print(\"⚠️ This may be due to sentence_transformers/transformers version conflict\")\n",
    "    print(\"💡 Try: pip install --upgrade sentence_transformers transformers\")\n",
    "    imports_successful = False\n",
    "\n",
    "try:\n",
    "    from src.rag_system.answer_generator import AnswerGenerator\n",
    "    print(\"✅ AnswerGenerator imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ AnswerGenerator import failed: {e}\")\n",
    "    imports_successful = False\n",
    "\n",
    "try:\n",
    "    from src.rag_system.integrated_rag import IntegratedRAG as RAG\n",
    "    print(\"✅ IntegratedRAG imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ IntegratedRAG import failed: {e}\")\n",
    "    imports_successful = False\n",
    "\n",
    "if imports_successful:\n",
    "    print(\"🎉 All RAG imports successful!\")\n",
    "else:\n",
    "    print(\"❌ Some imports failed. The notebook may not function correctly.\")\n",
    "    print(\"📝 Check your environment and package versions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = project_root / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "QA_PAIRS_DIR = DATA_DIR / \"qa_pairs\"\n",
    "CHUNKS_DIR = DATA_DIR / \"chunks\"\n",
    "INDEXES_DIR = DATA_DIR / \"indexes\"\n",
    "RAG_MODEL_DIR = project_root / \"models\" / \"rag\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "QA_PAIRS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INDEXES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAG_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Document Chunking\n",
    "\n",
    "In this step, we'll chunk the processed documents into smaller segments for retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the document chunker (only if imports were successful)\n",
    "if imports_successful and 'DocumentChunker' in globals():\n",
    "    try:\n",
    "        chunker = DocumentChunker(chunk_sizes=[100, 400], chunk_overlap=50)\n",
    "        print(\"✅ Document chunker initialized\")\n",
    "        \n",
    "        # List processed files\n",
    "        processed_files = list(PROCESSED_DIR.glob(\"*.txt\"))\n",
    "        print(f\"Found {len(processed_files)} processed files:\")\n",
    "        for file in processed_files:\n",
    "            print(f\"  - {file.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing chunker: {e}\")\n",
    "        chunker = None\n",
    "        processed_files = []\n",
    "else:\n",
    "    print(\"❌ Cannot initialize chunker - imports failed\")\n",
    "    chunker = None\n",
    "    processed_files = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk each processed file\n",
    "all_chunks = {}\n",
    "\n",
    "if processed_files:\n",
    "    for file_path in processed_files:\n",
    "        try:\n",
    "            print(f\"Chunking {file_path.name}...\")\n",
    "            chunks_by_size = chunker.chunk_file(file_path, output_dir=CHUNKS_DIR)\n",
    "            all_chunks[file_path.stem] = chunks_by_size\n",
    "            \n",
    "            # Print statistics for each chunk size\n",
    "            for size, chunks in chunks_by_size.items():\n",
    "                print(f\"  - Size {size}: {len(chunks)} chunks\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Error chunking {file_path.name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "else:\n",
    "    print(\"❌ No processed files found. Please run the data preprocessing notebook first.\")\n",
    "    print(\"Creating sample chunks for demonstration...\")\n",
    "    \n",
    "    # Create sample chunks if no files are available\n",
    "    sample_chunks = [\n",
    "        {\n",
    "            \"text\": \"ACME Corporation reported revenue of $1,250 million for fiscal year 2023, representing a 12% increase from the previous year.\",\n",
    "            \"chunk_id\": \"sample_1_chunk_1\",\n",
    "            \"source\": \"sample_document\",\n",
    "            \"chunk_size\": 100\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"The company's net profit margin improved to 15% in 2023, up from 14.4% in 2022, driven by operational efficiency improvements.\",\n",
    "            \"chunk_id\": \"sample_1_chunk_2\", \n",
    "            \"source\": \"sample_document\",\n",
    "            \"chunk_size\": 100\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Total assets increased to $3,500 million, with current assets of $1,800 million and non-current assets of $1,700 million.\",\n",
    "            \"chunk_id\": \"sample_1_chunk_3\",\n",
    "            \"source\": \"sample_document\", \n",
    "            \"chunk_size\": 100\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    all_chunks = {\"sample_document\": {400: sample_chunks}}\n",
    "    print(f\"Created {len(sample_chunks)} sample chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chunk length distribution\n",
    "chunk_lengths = []\n",
    "for file_name, chunks_by_size in all_chunks.items():\n",
    "    # Use the larger chunk size (400 tokens)\n",
    "    if 400 in chunks_by_size:\n",
    "        for chunk in chunks_by_size[400]:\n",
    "            chunk_lengths.append(len(chunk[\"text\"]))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(chunk_lengths, kde=True)\n",
    "plt.title(\"Chunk Length Distribution (400 token chunks)\")\n",
    "plt.xlabel(\"Length (characters)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total chunks: {len(chunk_lengths)}\")\n",
    "print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.1f} characters\")\n",
    "print(f\"Min chunk length: {min(chunk_lengths)} characters\")\n",
    "print(f\"Max chunk length: {max(chunk_lengths)} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Embeddings and Indexes\n",
    "\n",
    "Now, we'll create embeddings and indexes for efficient retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding manager\n",
    "try:\n",
    "    embedding_manager = EmbeddingManager(model_name=\"all-MiniLM-L6-v2\")\n",
    "    print(\"✅ Embedding manager initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing embedding manager: {e}\")\n",
    "    embedding_manager = None\n",
    "\n",
    "if embedding_manager:\n",
    "    # Load chunks (using the larger chunk size for better context)\n",
    "    chunk_size = 400\n",
    "    all_file_chunks = []\n",
    "    \n",
    "    # Combine chunks from all files\n",
    "    for file_name, chunks_by_size in all_chunks.items():\n",
    "        if chunk_size in chunks_by_size:\n",
    "            # Add file name to each chunk for tracking\n",
    "            for chunk in chunks_by_size[chunk_size]:\n",
    "                chunk[\"file\"] = file_name\n",
    "            \n",
    "            all_file_chunks.extend(chunks_by_size[chunk_size])\n",
    "    \n",
    "    print(f\"Total chunks to embed: {len(all_file_chunks)}\")\n",
    "    \n",
    "    # Build indexes\n",
    "    if all_file_chunks:\n",
    "        try:\n",
    "            print(\"Building indexes...\")\n",
    "            start_time = time.time()\n",
    "            embedding_manager.build_indexes(all_file_chunks)\n",
    "            end_time = time.time()\n",
    "            print(f\"✅ Indexes built in {end_time - start_time:.2f} seconds\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error building indexes: {e}\")\n",
    "            print(\"This is expected if LangChain components have compatibility issues.\")\n",
    "            print(\"The system will fall back to simpler retrieval methods.\")\n",
    "    else:\n",
    "        print(\"❌ No chunks available for indexing\")\n",
    "else:\n",
    "    print(\"❌ Cannot proceed without embedding manager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save indexes for future use\n",
    "print(\"Saving indexes...\")\n",
    "embedding_manager.save_indexes(INDEXES_DIR)\n",
    "print(f\"Indexes saved to {INDEXES_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Retrieval Methods\n",
    "\n",
    "Let's test different retrieval methods (dense, sparse, hybrid) to see which performs best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What was the revenue in the last fiscal year?\",\n",
    "    \"How much profit did the company make?\",\n",
    "    \"What are the total assets?\",\n",
    "    \"What is the debt to equity ratio?\",\n",
    "    \"How did sales compare to the previous year?\"\n",
    "]\n",
    "\n",
    "# Compare retrieval methods\n",
    "retrieval_methods = [\"dense\", \"sparse\", \"hybrid\"]\n",
    "top_k = 3\n",
    "\n",
    "# Function to display retrieval results\n",
    "def display_retrieval_results(query, results):\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        try:\n",
    "            print(f\"Result {i+1} (Score: {result['score']:.4f}, Method: {result['method']}):\")\n",
    "            print(f\"File: {result['chunk'].get('file', 'unknown')}\")\n",
    "            print(f\"Text: {result['chunk']['text'][:200]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying result {i+1}: {e}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Test each query with each retrieval method\n",
    "for query in test_queries:\n",
    "    print(f\"\\nTesting query: {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Answer Generation\n",
    "\n",
    "Now, let's set up the answer generation component using a small language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the answer generator\n",
    "print(\"Initializing answer generator...\")\n",
    "answer_generator = AnswerGenerator(model_name=\"distilgpt2\")\n",
    "print(\"Answer generator initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test answer generation\n",
    "test_query = \"What was the revenue in the last fiscal year?\"\n",
    "\n",
    "# Get retrieved chunks using hybrid retrieval\n",
    "retrieved_chunks = embedding_manager.hybrid_search(test_query, top_k=3)\n",
    "\n",
    "# Generate answer\n",
    "print(f\"Generating answer for: {test_query}\")\n",
    "try:\n",
    "    answer, confidence, response_time = answer_generator.generate_answer(test_query, retrieved_chunks)\n",
    "\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "    print(f\"Response time: {response_time:.3f}s\")\n",
    "\n",
    "    # Apply guardrails\n",
    "    modified_answer, is_hallucination = answer_generator.apply_guardrails(test_query, answer, retrieved_chunks)\n",
    "\n",
    "    if is_hallucination:\n",
    "        print(\"\\nHallucination detected! Modified answer:\")\n",
    "        print(modified_answer)\n",
    "    else:\n",
    "        print(\"\\nNo hallucination detected.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating answer: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Integrate the RAG System\n",
    "\n",
    "Finally, let's integrate all components into the complete RAG system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG system\n",
    "print(\"Initializing RAG system...\")\n",
    "rag_system = RAG(\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    llm_model=\"distilgpt2\",\n",
    "    chunk_sizes=[100, 400],\n",
    "    chunk_overlap=50,\n",
    "    retrieval_method=\"hybrid\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "# We'll use the embedding_manager and answer_generator we've already set up\n",
    "rag_system.embedding_manager = embedding_manager\n",
    "rag_system.answer_generator = answer_generator\n",
    "rag_system.is_initialized = True\n",
    "\n",
    "print(\"RAG system initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG system with sample queries\n",
    "test_queries = [\n",
    "    \"What was the revenue in the last fiscal year?\",\n",
    "    \"How much profit did the company make?\",\n",
    "    \"What are the total assets?\",\n",
    "    \"What is your favorite color?\"  # Irrelevant query to test guardrails\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        result = rag_system.process_query(query)\n",
    "        \n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "        print(f\"Response time: {result['response_time']:.3f}s\")\n",
    "        \n",
    "        if result['is_filtered']:\n",
    "            print(\"Query was filtered by input-side guardrails\")\n",
    "        elif result.get('is_hallucination', False):\n",
    "            print(\"Hallucination detected by output-side guardrails\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {e}\")\n",
    "    \n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save the RAG System\n",
    "\n",
    "Let's save the complete RAG system for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the RAG system\n",
    "print(\"Saving RAG system...\")\n",
    "rag_system.save(RAG_MODEL_DIR)\n",
    "print(f\"RAG system saved to {RAG_MODEL_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've implemented the complete RAG system for financial Q&A:\n",
    "\n",
    "1. Chunked the processed documents into smaller segments\n",
    "2. Created embeddings and indexes for efficient retrieval\n",
    "3. Tested different retrieval methods (dense, sparse, hybrid)\n",
    "4. Set up answer generation with a small language model\n",
    "5. Integrated all components into a complete RAG system\n",
    "6. Saved the system for later use\n",
    "\n",
    "The RAG system is now ready for evaluation and comparison with the Fine-Tuned model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
